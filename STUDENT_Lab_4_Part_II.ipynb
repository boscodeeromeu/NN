{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "STUDENT_Lab_4_Part_II.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s5y73IL66QG"
      },
      "source": [
        "# Lab 4 (Part 2): Character-Level LSTM in PyTorch\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "*Neural Networks. Bachelor in Data Science and Engineering*\n",
        "\n",
        "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
        "\n",
        "*Aurora Cobo Aguilera acobo@tsc.uc3m.es*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "In this notebook, construct a character-level LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina. **This model will be able to generate new text based on the text from the book!**\n",
        "\n",
        "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Below is the general architecture of the character-wise RNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1aMQ4GY66QJ"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "\n",
        "Image(url= \"http://karpathy.github.io/assets/rnn/charseq.jpeg\", width=500, height=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JG_HdBO66QJ"
      },
      "source": [
        "First let's load in our required resources for data loading and model creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frW3TMji66QK"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNhRE8b166QK"
      },
      "source": [
        "## Part I. Load in Data\n",
        "\n",
        "First, we load the Anna Karenina text file and convert it into integers for our network to use. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNCe_QzG-xL"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB8odWPK66QK"
      },
      "source": [
        "# open text file and read in data as `text`\n",
        "with open('anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kfCGoEZ66QK"
      },
      "source": [
        "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V81mNU4m66QL"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOqT1u3n66QL"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network.\n",
        "\n",
        "Your can read more about Python **set data types** in this [link](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset). Also, check [here](http://book.pythontips.com/en/latest/enumerate.html) about the `enumerate()` Python built-in function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL45EfD566QM"
      },
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars = tuple(set(text))             #set() is an unordered collection of unique elements in text\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwsNj_ee66QN"
      },
      "source": [
        "And we can see those same characters from above, encoded as integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ynxCtbr66QN"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J21dH--c66QO"
      },
      "source": [
        "## Part II. Pre-processing the data\n",
        "\n",
        "As you can see in our char-RNN image above, our LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since we're one-hot encoding the data, let's make a function to do that!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45dXAIIs66QP"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    num_elements = arr.shape[0]*arr.shape[1]\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros([num_elements, n_labels], dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.reshape([num_elements])] = 1.\n",
        "    \n",
        "    # Reshape as (n_batch,seq_length,n_labels)\n",
        "    \n",
        "    return one_hot.reshape(arr.shape[0],arr.shape[1],n_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixroLXD_66QP"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1],[2,4,6]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tTVBSM366QQ"
      },
      "source": [
        "## Part III. Making training mini-batches\n",
        "\n",
        "\n",
        "To train on this data, we also want to create mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsWVtmLG66QQ"
      },
      "source": [
        "Image(url= \"http://digtime.cn/uploads/images/201907/12/1/Tt94B4Htu7.png\", width=700, height=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XCZ8ak766QQ"
      },
      "source": [
        "In this example, we'll take the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of our sequences will be `seq_length` long.\n",
        "\n",
        "### Creating Batches\n",
        "\n",
        "**1. The first thing we need to do is discard some of the text so we only have completely full mini-batches. **\n",
        "\n",
        "Each batch contains $N \\times M$ characters, where **$N$ is the batch size** (the number of sequences in a batch) and $M$ is the seq_length or number of time steps in a sequence. Then, to get the total number of batches, $K$, that we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n",
        "\n",
        "**2. After that, we need to split `arr` into batches. ** \n",
        "\n",
        "You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences in a batch, so let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n",
        "\n",
        "**3. Now that we have this array, we can iterate through it to get our mini-batches. **\n",
        "\n",
        "The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `seq_length`. We also want to create both the input and target arrays. Remember that the targets are just the inputs shifted over by one character. \n",
        "\n",
        "> **Exercise:** Understand the following code for creating batches in the function below. **Note:** Take a look to this [link](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do) to understand what `yield` does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY9ssC9x66QR"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))      # IMPORTANT TO UNDERSTAND THE DIMENSIONS OF arr\n",
        "    \n",
        "    # iterate through the array. We read every of the n_batches sequeces in windows of seq_length characters\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        \n",
        "        if (n+seq_length<arr.shape[1]):     \n",
        "            y[:,-1] = arr[:, n+seq_length]\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "        else: # If n+seq_length == arr.shape[1], then we are done! (arr[:, n+seq_length] yields an indexing error).\n",
        "              # We return a window with one column less (as the target for the last character is not available)\n",
        "            x = arr[:, n:n+seq_length-1]\n",
        "            y = np.zeros_like(x)\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:,-1] = arr[:,-1]            \n",
        "        yield x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGPmKRpW66QR"
      },
      "source": [
        "### Test Your Implementation\n",
        "\n",
        "Lets make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 8 and 50 sequence steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn2JhtIE66QS"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x,y = next(batches)\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OlyguhVh66QS"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kloPmLBN66QS"
      },
      "source": [
        "The above output should look something like \n",
        "```\n",
        "x\n",
        " [[25  8 60 11 45 27 28 73  1  2]\n",
        " [17  7 20 73 45  8 60 45 73 60]\n",
        " [27 20 80 73  7 28 73 60 73 65]\n",
        " [17 73 45  8 27 73 66  8 46 27]\n",
        " [73 17 60 12 73  8 27 28 73 45]\n",
        " [66 64 17 17 46  7 20 73 60 20]\n",
        " [73 76 20 20 60 73  8 60 80 73]\n",
        " [47 35 43  7 20 17 24 50 37 73]]\n",
        "\n",
        "y\n",
        " [[ 8 60 11 45 27 28 73  1  2  2]\n",
        " [ 7 20 73 45  8 60 45 73 60 45]\n",
        " [20 80 73  7 28 73 60 73 65  7]\n",
        " [73 45  8 27 73 66  8 46 27 65]\n",
        " [17 60 12 73  8 27 28 73 45 27]\n",
        " [64 17 17 46  7 20 73 60 20 80]\n",
        " [76 20 20 60 73  8 60 80 73 17]\n",
        " [35 43  7 20 17 24 50 37 73 36]]\n",
        " ```\n",
        " although the exact numbers may be different. Check to make sure the data is shifted over one step for `y`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAFt6NRU66QS"
      },
      "source": [
        "---\n",
        "## Part IV. Defining the network with PyTorch\n",
        "\n",
        "Below is where you'll define the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgoJOLgP66QT"
      },
      "source": [
        "Image(url= \"http://digtime.cn/uploads/images/201907/12/1/ZG9JIjidmz.png\", width=500, height=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRYGGXTx66QT"
      },
      "source": [
        "#### Model Structure\n",
        "\n",
        "In `__init__` the suggested structure is as follows:\n",
        "* Create and store the necessary dictionaries \n",
        "* Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout probability `drop_prob`, and a batch_first boolean (True, since we are batching)\n",
        "* Define a dropout layer with `drop_prob`\n",
        "* Define a fully-connected layer with params: input size `n_hidden` and output size (the number of characters)\n",
        "* Finally, initialize LSTM hidden state (h_0) and memory (c_0)\n",
        "\n",
        "\n",
        "#### LSTM Inputs/Outputs\n",
        "\n",
        "You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. And we can add dropout by adding a dropout parameter with a specified probability; this will automatically add dropout to the inputs or outputs. \n",
        "\n",
        "\n",
        "> **Exercise:** Complete the following code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCgx35K466QT"
      },
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_chars, hidden_dim=256, n_layers=2,drop_prob=0.5):\n",
        "        \n",
        "        ''' n_chars is the number of different characters '''        \n",
        "        \n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim        \n",
        "        \n",
        "        ## TODO: define the LSTM. Do not forget to include the dropout between LSTM layers!\n",
        "        self.lstm = nn.LSTM(#YOUR CODE HERE)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = #YOUR CODE HERE)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = #YOUR CODE HERE)\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1) \n",
        "      \n",
        "    \n",
        "    def forward(self, x, h=None):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the LSTM initial hidden/cell state h. \n",
        "            When h is not provided, default initializaiton (all zeros) is used.'''\n",
        "        \n",
        "        if (h==None):\n",
        "            #No initial hidden_state and memory are provided. They are set to 0s\n",
        "            r_output, hidden = self.lstm(x)  \n",
        "        \n",
        "        else:\n",
        "            #Non-zero initial state. This is used for generating new text\n",
        "            r_output, hidden = self.lstm(x,h)     \n",
        "        \n",
        "        ## TODO: pass through a dropout layer\n",
        "        out = #YOUR CODE HERE)\n",
        "        \n",
        "        # Stack up LSTM outputs\n",
        "        out = out.reshape(-1, self.hidden_dim) \n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer and a logsoftmax output\n",
        "        out = #YOUR CODE HERE)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD16lkxW66QU"
      },
      "source": [
        "The following code extends the above class by including a training method, and also the methods to perform one-hot encoding and get the mini-batches from the text. A couple of details about training: \n",
        "* Within the batch loop, we detach the hidden state from its history; this time setting it equal to a new *tuple* variable because an LSTM has a hidden state that is a tuple of the hidden and cell states.\n",
        "* We use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients.\n",
        "\n",
        "> **Exercise:** Complete the following code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSlTGPIn66QU"
      },
      "source": [
        "class CharLSTM_extended(CharLSTM):\n",
        "    \n",
        "    \n",
        "    def __init__(self, batch_size, sequence_length, chars, hidden_dim=256, n_layers=2, \n",
        "                 epochs=5, drop_prob=0.5, lr=0.001,clip=5):\n",
        "        \n",
        "        self.n_char = len(chars)\n",
        "            \n",
        "        super().__init__(self.n_char, hidden_dim, n_layers,drop_prob)  \n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.clip = clip\n",
        "        \n",
        "        self.seq_length = sequence_length\n",
        "        \n",
        "        self.num_layers = n_layers\n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "    \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()             \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "        # Setting GPU training if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.to(self.device)\n",
        "        \n",
        "        # Training mode by default\n",
        "        self.train()\n",
        "        \n",
        "    def get_batches(self,arr):\n",
        "        '''Create a generator that returns batches of size\n",
        "           batch_size x seq_length from arr.\n",
        "\n",
        "           Arguments\n",
        "           ---------\n",
        "           arr: Array you want to make batches from\n",
        "           batch_size: Batch size, the number of sequences per batch\n",
        "           seq_length: Number of encoded chars in a sequence\n",
        "        '''\n",
        "        \n",
        "        #YOUR CODE HERE\n",
        "        \n",
        "            \n",
        "    def one_hot_encode(self,arr):\n",
        "        \n",
        "        #YOUR CODE HERE\n",
        "    \n",
        "    \n",
        "    def trainloop(self,encoded_text):\n",
        "        \n",
        "        for e in range(self.epochs):\n",
        "            \n",
        "            counter = 0.\n",
        "            \n",
        "            start_time = time.time()\n",
        "            \n",
        "            running_loss = 0.\n",
        "            \n",
        "            for x, y in self.get_batches(encoded_text):\n",
        "                \n",
        "                counter += 1.\n",
        "                \n",
        "                # One-hot encode our data and make them Torch tensors\n",
        "                x = self.one_hot_encode(x)\n",
        "\n",
        "                x, y = torch.from_numpy(x).to(self.device), torch.from_numpy(y).to(self.device)\n",
        "                \n",
        "                # TO DO: reset gradients \n",
        "                #YOUR CODE HERE\n",
        "                \n",
        "                # TO DO: compute output\n",
        "                #YOUR CODE HERE\n",
        "                \n",
        "                # Calculate the loss and perform backprop\n",
        "                loss = self.criterion(out, y.reshape([out.shape[0],])) \n",
        "                # TO DO: compute gradients\n",
        "                #YOUR CODE HERE)\n",
        "                \n",
        "                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "                nn.utils.clip_grad_norm_(self.parameters(), self.clip)\n",
        "                \n",
        "                self.optim.step() \n",
        "                \n",
        "                running_loss += loss.item()\n",
        "                \n",
        "            self.loss_during_training.append(running_loss/counter)\n",
        "            \n",
        "            if(e % 1 == 0): \n",
        "                \n",
        "                print(\"Epoch %d. Training loss: %f, Time per epoch: %f seconds\" \n",
        "                      %(e,self.loss_during_training[-1],(time.time() - start_time)))            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3zIaC-V66QU"
      },
      "source": [
        "my_charLSTM = CharLSTM_extended(batch_size=128,sequence_length=100,chars=chars,\n",
        "                                epochs=10,hidden_dim=512,n_layers=2,drop_prob=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyCJhw_j66QV"
      },
      "source": [
        "my_charLSTM.trainloop(encoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHODUOwL66QV"
      },
      "source": [
        "## Part V. Getting the best model\n",
        "\n",
        "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network.\n",
        "\n",
        "> **Exercise:** \n",
        "> - Separate the text intro training text and validation text.\n",
        "> - Modify the class CharLSTM_extended that monitors the validation loss during training\n",
        "> - Save your model after every epoch (save the parameters in a different file for every epoch). To do so, include a method in the class to save the model parameters. Then, you call it after the end of every epoch.\n",
        "> - Perform early stopping. I.e., load the parameters of the last epoch in which validation loss was decreasing.\n",
        "\n",
        "Here's some good advice from [Andrej Karpathy](https://github.com/karpathy/char-rnn#tips-and-tricks) on training the network.\n",
        "\n",
        "In order to save at every epoch, the following code can help. Here we are saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters.\n",
        "\n",
        "```python\n",
        "\n",
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_' + str(e) + '_epoch.net'\n",
        "\n",
        "checkpoint = {'hidden_dim': my_charLSTM.hidden_dim,\n",
        "              'n_layers': my_charLSTM.n_layers,\n",
        "              'batch_size': my_charLSTM.batch_size,\n",
        "              'sequence_length': my_charLSTM.seq_length,\n",
        "              'state_dict': my_charLSTM.state_dict(),\n",
        "              'chars': my_charLSTM.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)\n",
        "```\n",
        "\n",
        "Then, if you want to restore your model, you can use the following code\n",
        "\n",
        "```python\n",
        "\n",
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharLSTM_extended(batch_size=checkpoint['batch_size'],seq_length=checkpoint['seq_length'], \n",
        "                           chars=checkpoint['chars'],\n",
        "                           hidden_dim=checkpoint['hidden_dim'], \n",
        "                           n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veqNP3eS66QV"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UePta7XW66QV"
      },
      "source": [
        "---\n",
        "## Part VI. Making Predictions\n",
        "\n",
        "Now that the model is trained, we'll want to sample from it and make predictions about next characters! To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
        "\n",
        "### A note on the `predict`  function\n",
        "\n",
        "The output of our RNN is from a fully-connected layer followed by a Softmax layer, and it outputs a **distribution of next-character scores**.\n",
        "\n",
        "### Top K sampling\n",
        "\n",
        "Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. Read more about [topk, here](https://pytorch.org/docs/stable/torch.html#torch.topk).\n",
        "\n",
        "> **Exercise:** Complete the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca-Lq7Q666QW"
      },
      "source": [
        "def predict(charLSTM_class, current_char, dict_c2int,h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[char2int[current_char]]])\n",
        "        x = charLSTM_class.one_hot_encode(x)\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        charLSTM_class.to('cpu')\n",
        "    \n",
        "        # TO DO: get the output of the model\n",
        "        #YOUR CODE HERE\n",
        "\n",
        "        p, top_ch = torch.exp(out).topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.detach().numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return int2char[char], h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udwvm-6r66QW"
      },
      "source": [
        "### Generating text from an initial string\n",
        "\n",
        "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from.\n",
        "\n",
        "> **Exercise:** Understand the following code! Nothing to do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV0R5Iq66QW"
      },
      "source": [
        "def sample(charLSTM_class, size, dict_c2int, prime='The', top_k=5):\n",
        "    \n",
        "    charLSTM_class.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = None\n",
        "    for ch in prime:\n",
        "        char, h = predict(charLSTM_class, ch, char2int, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(charLSTM_class, chars[-1], char2int, h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrPYEJaW66QW"
      },
      "source": [
        "Finally, lets take a look to the text that our model generates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_uGydcp66QW"
      },
      "source": [
        "print(sample(my_charLSTM, 1000, char2int, prime='Anna', top_k=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiFv0hXd66QW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
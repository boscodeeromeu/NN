{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "STUDENT_Lab_3_Part_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "layB9pEvUwqf"
      },
      "source": [
        "# Lab 3 Part 2: Batch Normalization \n",
        "\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "*Neural Networks. Bachelor in Data Science and Engineering*\n",
        "\n",
        "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
        "\n",
        "*Aurora Cobo Aguilera acobo@tsc.uc3m.es*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to _layers within_ the network. \n",
        "\n",
        "> It's called **batch** normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current *batch*.\n",
        "\n",
        "We will first analyze the effect of Batch Normalization (BN) in a simple NN with dense layers. Then you will be able to incorportate BN into the CNN that you designed in the first part of Lab 3. \n",
        "\n",
        "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siyfhjMsUwqh"
      },
      "source": [
        "## Batch Normalization in PyTorch<a id=\"implementation_1\"></a>\n",
        "\n",
        "This section of the notebook shows you one way to add batch normalization to a neural network built in PyTorch. \n",
        "\n",
        "The following cells import the packages we need in the notebook and load the MNIST dataset to use in our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FN3FRcAUwqi"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORTgEv3CUwqr"
      },
      "source": [
        "### Run this cell\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training  data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlate1DnUwqx"
      },
      "source": [
        "### Neural network classes\n",
        "\n",
        "The following class, `MLP`, allows us to create identical neural networks **with and without batch normalization** to compare. We are defining a simple NN with **two dense layers** for classification; this design choice was made to support the discussion related to batch normalization and not to get the best classification accuracy.\n",
        "\n",
        "Two importants points about BN:\n",
        "\n",
        "- We use PyTorch's [BatchNorm1d](https://pytorch.org/docs/stable/nn.html#batchnorm1d). This is the function you use to operate on linear layer outputs; you'll use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) for 2D outputs like filtered images from convolutional layers. \n",
        "- We add the batch normalization layer **before** calling the activation function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OW8ZWM1Uwqz"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,dimx,hidden1,hidden2,nlabels,use_batch_norm): #Nlabels will be 10 in our case\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # Keep track of whether or not this network uses batch normalization.\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        \n",
        "        self.output1 = nn.Linear(dimx,hidden1)\n",
        "        \n",
        "        self.output2 = nn.Linear(hidden1,hidden2)        \n",
        "        \n",
        "        self.output3 = nn.Linear(hidden2,nlabels)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        if self.use_batch_norm:\n",
        "\n",
        "            self.batch_norm1 = nn.BatchNorm1d(hidden1)\n",
        "            \n",
        "            self.batch_norm2 = nn.BatchNorm1d(hidden2)\n",
        "            \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.output1(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output2(x)\n",
        "        if self.use_batch_norm:\n",
        "            x = self.batch_norm2(x)        \n",
        "        x = self.relu(x)\n",
        "        x = self.output3(x)\n",
        "        x = self.logsoftmax(x) \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DRvyjGMUwq3"
      },
      "source": [
        "> **Exercise:** \n",
        "> \n",
        "> - Create a validation set with the 20% of training set\n",
        "> - Extend the class above to incorporate a training method where both training and validation losses are computed, and a method to evaluate the classification performance on a given set\n",
        "\n",
        "**Note:** As we do with Dropout, for BN we have to call the methods `self.eval()` and `self.train()` in both validation and training. Setting a model to evaluation mode is important for models with batch normalization layers!\n",
        "\n",
        ">* Training mode means that the batch normalization layers will use **batch** statistics to calculate the batch norm. \n",
        "* Evaluation mode, on the other hand, uses the estimated **population** mean and variance from the entire training set, which should give us increased performance on this test data!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAT1IgvUwq4"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nloclzQLUwrB"
      },
      "source": [
        "### Create two different models for testing\n",
        "\n",
        "* `net_batchnorm` uses batch normalization applied to the output of its hidden layers\n",
        "* `net_no_norm` does not use batch normalization\n",
        "\n",
        "Besides the normalization layers, everthing about these models is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFQ2pcH8UwrG"
      },
      "source": [
        "> **Exercise:** Train both models and compare the evolution of the train/validation loss in both cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfjYlvmHasYs"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfyqTIjVUwrR"
      },
      "source": [
        "---\n",
        "### Considerations for other network types\n",
        "\n",
        "This notebook demonstrates batch normalization in a standard neural network with fully connected layers. You can also use batch normalization in other types of networks, but there are some special considerations.\n",
        "\n",
        "#### ConvNets\n",
        "\n",
        "Convolution layers consist of multiple feature maps. (Remember, the depth of a convolutional layer refers to its number of feature maps.) And the weights for each feature map are shared across all the inputs that feed into the layer. Because of these differences, batch normalizing convolutional layers requires batch/population mean and variance per feature map rather than per node in the layer.\n",
        "\n",
        "> To apply batch normalization on the outputs of convolutional layers, we use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d). To use it, we simply state the **number of input feature maps**. I.e. `nn.BatchNorm2d(num_features=nmaps)`\n",
        "\n",
        "\n",
        "#### RNNs\n",
        "\n",
        "Batch normalization can work with recurrent neural networks, too, as shown in the 2016 paper [Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025). It's a bit more work to implement, but basically involves calculating the means and variances per time step instead of per layer. You can find an example where someone implemented recurrent batch normalization in PyTorch, in [this GitHub repo](https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXRabJ90UwrT"
      },
      "source": [
        "> **Exercise:** Using CIFAR10 database, incorporate BN to your solution of Lab 3 (Part I). Compare the results with and without BN!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLHhFPGpXQLk"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
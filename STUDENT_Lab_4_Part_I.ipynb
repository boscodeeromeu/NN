{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "STUDENT_Lab_4_Part_I.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx29UUPe6vwJ"
      },
      "source": [
        "# Lab 4 (Part 1): RNNs for Time Series Prediction\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "*Neural Networks. Bachelor in Data Science and Engineering*\n",
        "\n",
        "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
        "\n",
        "*Aurora Cobo Aguilera acobo@tsc.uc3m.es*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "In this notebook, you will deploy a simple probabilistic model using a RNN to infer the probability distribution of a time-series. Given a set of signals $\\mathcal{D}=\\{X^1,X_2,\\ldots,X_N\\}$ where \n",
        "$$X^i = [X_0^i,X_1^i,\\ldots,X_T^i]$$\n",
        "is the $i$-th signal, we will train a probabilistic model of the form\n",
        "$$p(X|X_0) = \\prod_{t=0}^T p(X_t|X_{t-1},X_{:t-1})$$\n",
        "where each factor is a Gaussian distribution with variance $\\sigma^2$ and mean given by **the ouput of a RNN** with input $X_{t-1}$. Hence $X_{:t-1}$ in the conditional probability $p(X_t|X_{t-1},X_{:t-1})$ is embedded through the **RNN** state $\\mathbf{h}_{t-1}$:\n",
        "\n",
        "$$p(X_t|X_{t-1},X_{:t-1}) = \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2\\right)$$\n",
        "\n",
        "During training, for $t=1,\\ldots,T$, we will sample $\\hat{X}_t$ from $p(X_t|X_{t-1},X_{:t-1})$ and minimize the average square loss $\\frac{1}{T}\\sum_{t=1}^T(X_t-\\hat{X}_t)^2$. Then we average again for all signals in the training set. Note that during training we **feed the RNN with the true values of the signal** $X^i = [X_0^i,X_1^i,\\ldots,X_T^i]$. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2ObJAUj6vwL"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTerxwEK6vwM"
      },
      "source": [
        "## Part I. Create a synthetic database\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We will generate $N$ target signals of length $T$ time steps. We generate each signal as one realization of the following autoregressive model\n",
        "\\begin{align}\n",
        "X_{t}=c+\\sum_{i=1}^{p} \\varphi_{i} X_{t-i}+\\varepsilon_{t}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "4GpspZZB6vwM"
      },
      "source": [
        "N = 1000 # Number of signals\n",
        "\n",
        "T = 200\n",
        "\n",
        "c = 0\n",
        "\n",
        "phi_1 = 1\n",
        "\n",
        "phi_2 = -1\n",
        "\n",
        "phi_3 = 1\n",
        "\n",
        "sigma = 1\n",
        "\n",
        "X = np.zeros([N,T])\n",
        "\n",
        "np.random.seed(23)\n",
        "\n",
        "X[:,0] = c + np.random.randn(N,)*np.sqrt(sigma)\n",
        "\n",
        "X[:,1] = c + phi_1 * X[:,0] + np.random.randn(N,)*np.sqrt(sigma)\n",
        "\n",
        "X[:,2] = c + phi_1 * X[:,1] + phi_2 * X[:,0] + np.random.randn(N,)*np.sqrt(sigma)\n",
        "\n",
        "X[:,3] = c + phi_1 * X[:,2] + phi_2 * X[:,1] + phi_3 * X[:,0] + np.random.randn(N,)*np.sqrt(sigma)\n",
        "\n",
        "t = 4\n",
        "\n",
        "while (t<T):\n",
        "\n",
        "    X[:,t] = c + phi_1 * X[:,t-1] + phi_2 * X[:,t-2] + phi_3 * X[:,t-3] + np.random.randn(N,)*np.sqrt(sigma)\n",
        "    \n",
        "    t +=1\n",
        "    \n",
        "\n",
        "# Create targets\n",
        "\n",
        "Y = X[:,1:] # all but the first\n",
        "X = X[:,:-1] # all but the last piece of data\n",
        "\n",
        "T -=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykzIdBNa6vwN"
      },
      "source": [
        "The goal of the RNN will be to predict the value of the signal in the next time point given the current observation. Note that the noise in the model\n",
        "\n",
        "$$p(X_t|X_{t-1},X_{:t-1}) = \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2\\right)$$\n",
        "\n",
        "will simply introduce an error that will prevent the model from overfitting. \n",
        "\n",
        "Lets plot one of the signals versus the *target*, which is the same signal but shifted to the right ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIhbzirs6vwN"
      },
      "source": [
        "# Plot the signal \n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(T), X[1,:T], 'r.--', label='input, x',ms=10) # x\n",
        "plt.plot(np.arange(T), Y[1,:T], 'b.-', label='target, y',ms=10) # y\n",
        "\n",
        "plt.legend(loc='best')\n",
        "\n",
        "# Plot the signal (20 first steps)\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(20), X[1,:20], 'r.--', label='input, x',ms=10) # x\n",
        "plt.plot(np.arange(20), Y[1,:20], 'b.-', label='target, y',ms=10) # y\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj_-6z-z6vwO"
      },
      "source": [
        "## Part II. Define RNN\n",
        "\n",
        "Next, we define an RNN in PyTorch. We'll use `nn.RNN` to create an RNN layer, which takes in a number of parameters:\n",
        "* **input_size** - the size of the input\n",
        "* **hidden_dim** - the dimension of the RNN output and the hidden state\n",
        "* **n_layers** - the number of layers that make up the RNN, typically 1-3; greater than 1 means that you'll create a **stacked RNN** \n",
        "\n",
        "\n",
        "This is an example of a stacked RNN\n",
        "\n",
        "<img src=\"https://yiyibooks.cn/__src__/wizard/nmt-tut-neubig-2017_20180721165003_deleted/img/6-5.jpg\" width=\"40%\"> \n",
        "\n",
        "\n",
        "If you take a look at the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn), you will see that `nn.RNN` only provides the actual computation of the hidden states along time\n",
        "\\begin{align}\n",
        "h_{t}=g \\left(W_{i h} x_{t}+b_{i h}+W_{h h} h_{(t-1)}+b_{h h}\\right)\n",
        "\\end{align}\n",
        "\n",
        "Then we'll add a last, fully-connected layer to get the output size that we want. For simplicity, **the input to this dense layer is the state $h_t$ of the RNN**.\n",
        "\n",
        "You have to pay special attention to the dimensions of the input/output tensors of the RNN. **Check the [RNN documentation](https://pytorch.org/docs/stable/nn.html#rnn)**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Admz7Hx96vwO"
      },
      "source": [
        "The following class implements a class where \n",
        "- An input signal of dimension `input_size` is processed by a RNN. As a result, we obtain a sequence of states $\\mathbf{h}_{t}$, from $t=1$ to $t=T$.\n",
        "- We process each state with a linear layer to estimate the output signal (of dimension `output_size`) at time $t$ from $\\mathbf{h}_{t}$. \n",
        "- We sum a Gaussian noise of variance `sigma` to the output of the linear layer.\n",
        "\n",
        "> **Exercise**: complete the following code. Understand all steps, particularly those in the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKlZh6zV6vwP"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers,sigma):\n",
        "        \n",
        "        # input size -> Dimension of the input signal\n",
        "        # outpusize -> Dimension of the output signal\n",
        "        # hidden_dim -> Dimension of the rnn state\n",
        "        # n_layers -> If >1, we are using a stacked RNN\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        \n",
        "        self.sigma = sigma\n",
        "\n",
        "        # define an RNN with specified parameters\n",
        "        # batch_first=True means that the first dimension of the input will be the batch_size\n",
        "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_dim, num_layers=n_layers, \n",
        "                          nonlinearity='relu',batch_first=True)\n",
        "        \n",
        "        # last, fully-connected layer\n",
        "        self.fc1 = nn.Linear(# YOUR CODE HERE) \n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        \n",
        "        '''\n",
        "        About the shape of the different tensors ...:\n",
        "        \n",
        "        - Input signal x has shape (batch_size, seq_length, input_size)\n",
        "        - The initialization of the RNN hidden state h0 has shape (n_layers, batch_size, hidden_dim).\n",
        "          If None value is used, internally it is initialized to zeros.\n",
        "        - The RNN output (batch_size, seq_length, hidden_size). This output is the RNN state along time  \n",
        "\n",
        "        '''\n",
        "        \n",
        "        batch_size = x.size(0) # Number of signals N\n",
        "        seq_length = x.size(1) # T\n",
        "        \n",
        "        # get RNN outputs\n",
        "        # r_out is the sequence of states\n",
        "        # hidden is just the last state (we will use it for forecasting)\n",
        "        r_out, hidden = self.rnn(x, h0)\n",
        "        \n",
        "        # shape r_out to be (seq_length, hidden_dim) #UNDERSTANDING THIS POINT IS IMPORTANT!!        \n",
        "        r_out = r_out.reshape(-1, self.hidden_dim) \n",
        "        \n",
        "        output = self.fc1(r_out)\n",
        "        \n",
        "        noise = torch.randn_like(output)*sigma\n",
        "        \n",
        "        output += noise\n",
        "        \n",
        "        # reshape back to temporal structure\n",
        "        output = output.reshape([-1,seq_length,1])\n",
        "        \n",
        "        return output, hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9VZxaaE6vwP"
      },
      "source": [
        "> **Exercise:** Instantiate the object RNN with the right parameters for our problem. Use `hidden_dim=32`, `n_layers=1` and `sigma=1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQsybOcp6vwQ"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFls2PT6vwQ"
      },
      "source": [
        "In the following code, we compute the model output using the `forward` method. Note that we use an all zero initial state.\n",
        "\n",
        "> **Exercise**: Complete the following code. What are the dimensions of variables `h` and `o`? How are these dimensions related to the number of signals, hidden state of the RNN and signal duration?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD-6Xpct6vwQ"
      },
      "source": [
        "X_in = torch.Tensor(X).view([-1,T,1])\n",
        "\n",
        "o,h = test_rnn.forward(# YOUR CODE HERE)\n",
        "\n",
        "\n",
        "print(h.shape)\n",
        "\n",
        "print(o.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbE6BER76vwR"
      },
      "source": [
        "> **Exercise:** Complete the code for the following class, which extends `RNN` to include a training method. \n",
        "\n",
        "Note that there is no mini-batch, we process all signals for every SGD iteration. You are free to the mini-batch training functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xkbb690f6vwR"
      },
      "source": [
        "class RNN_extended(RNN):\n",
        "    \n",
        "    #Your code here\n",
        "    \n",
        "    def __init__(self, num_data_train, num_iter, sequence_length,\n",
        "                 input_size, output_size, hidden_dim, n_layers, sigma, lr=0.001):\n",
        "        \n",
        "        super().__init__(input_size, output_size, hidden_dim, n_layers,sigma) \n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "        self.num_layers = n_layers\n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.num_train = num_data_train #Number of training signals\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.num_iter = num_iter\n",
        "        \n",
        "        self.criterion = #YOUR CODE HERE     \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "           \n",
        "    def trainloop(self,x,y):\n",
        "        \n",
        "        # SGD Loop\n",
        "        \n",
        "        for e in range(int(self.num_iter)):\n",
        "        \n",
        "            self.optim.zero_grad() \n",
        "                \n",
        "            x = torch.Tensor(x).view([self.num_train,#YOUR CODE HERE ,1])  \n",
        "\n",
        "            y = torch.Tensor(y).view([self.num_train,#YOUR CODE HERE ,1])   \n",
        "\n",
        "            out,hid = self.forward(x)\n",
        "                \n",
        "            loss = self.criterion(#YOUR CODE HERE) \n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            # This code helps to avoid vanishing exploiting gradients in RNNs\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
        "                \n",
        "            self.optim.step()\n",
        "                \n",
        "            self.loss_during_training.append(loss.item()/self.num_iter)\n",
        "\n",
        "            if(e % 50 == 0): # Every 10 iterations\n",
        "\n",
        "                print(\"Iteration %d. Training loss: %f\" %(e,self.loss_during_training[-1]))                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgwd-xSe6vwR"
      },
      "source": [
        "> **Exercise:** Using only the first 100 values of every signal, train the RNN for 100 SGD iterations. Use `hidden_dim=32`, `n_layers=1`,`lr=0.005`, and `sigma=1`. Recall that the target signal is stored in the variable `Y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrODxHBd6vwS"
      },
      "source": [
        "T_train = 100\n",
        "\n",
        "my_rnn = RNN_extended(num_data_train=X.shape[0],num_iter=200,sequence_length=T_train,\n",
        "                     input_size=1,output_size=1,hidden_dim=32,n_layers=1,sigma=1.0,lr=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lNYMW37q6vwS"
      },
      "source": [
        "my_rnn.trainloop(X[:,:T_train],Y[:,:T_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEnT4U9C6vwS"
      },
      "source": [
        "> **Exercise:** Plot the loss function along training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP0VXb0H6vwS"
      },
      "source": [
        "plt.plot(my_rnn.loss_during_training,label='Training Loss')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHpA3dij6vwT"
      },
      "source": [
        "> **Exercise:** Compute the following plot, in which we plot one of the input signals, the target one, and the predicted by the RNN from t=50 to t=100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbyFZp0u6vwT"
      },
      "source": [
        "# We first evaluate the model for the N signals up to time T_train = 100\n",
        "X_in = torch.Tensor(X[:,:T_train]).view(# YOUR CODE HERE) \n",
        "\n",
        "o,h = my_rnn.forward(# YOUR CODE HERE) \n",
        "\n",
        "output_rnn = o.detach().numpy().reshape([N,-1])\n",
        "\n",
        "offset = 50\n",
        "\n",
        "signal = 0 # From 1 to N (you can play with this)\n",
        "\n",
        "# Plot the first training signal and the target\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(T_train-offset,T_train,1), X[signal,T_train-offset:T_train], 'r.--', label='input, x',ms=10) # x\n",
        "plt.plot(np.arange(T_train-offset,T_train,1), Y[signal,T_train-offset:T_train], 'b.-', label='target, y',ms=10) # x\n",
        "plt.plot(np.arange(T_train-offset,T_train,1), output_rnn[signal,T_train-offset:T_train], 'k.-', label='RNN output',ms=10) # y\n",
        "\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ghDRsLt6vwT"
      },
      "source": [
        "Observe that the prediction is pretty good!The RNN model have clearly learnt the dynamics of the dataset. In the previous experiment, note we have fed the RNN model with the **true** values of the signal, i.e. we have used the full signal $X$ to compute the sequence of states. \n",
        "\n",
        "Using the model we have just trained, lets do now **forecasting**. Namely, we feed the RNN the output that we predicted and we do this recursively for as long as we want. This represents **sampling** from the probabilistic model \n",
        "\n",
        "\n",
        "\n",
        "$$p(X|X_{:T_{train}}) = \\prod_{t=T_{train}}^T \\mathcal{N}\\left(f_{RNN}(X_{t-1},\\mathbf{h}_{t-1}),\\sigma^2\\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHs2gq3J6vwT"
      },
      "source": [
        "To do forecasting, note that we have to recursively call the `forward` method and feed the obtained RNN output and state as the entry and initial state for the next `forward` call. The following code would do the job:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8yGGZlS6vwT"
      },
      "source": [
        "# We take the last RNN output \n",
        "current_input = o[:,-1,:].view([N,1,1]) #Note that current input only contains one observation for each of the N signals\n",
        "# We take the last RNN state\n",
        "current_state = h\n",
        "\n",
        "forecast_rnn = np.zeros([N,T-T_train])\n",
        "\n",
        "for t in range(T-T_train):\n",
        "    \n",
        "    # ... and feed them as input and initial state\n",
        "    \n",
        "    current_input,current_state = my_rnn.forward(current_input,current_state)\n",
        "    \n",
        "    forecast_rnn[:,t] = current_input.detach().numpy().reshape([-1,])\n",
        "    \n",
        "final_rnn_reconstruct = np.hstack([output_rnn,forecast_rnn])\n",
        "\n",
        "# We plot the signal and the target before and after forecasting\n",
        "\n",
        "plt.plot(np.arange(0,T-1,1), Y[signal,:-1].reshape([-1]), 'b.-', label='target, y',ms=10) \n",
        "plt.plot(np.arange(0,T-1,1), final_rnn_reconstruct[signal,:-1], 'g.-', label='RNN output',ms=10) \n",
        "plt.plot([T_train,T_train],[np.min(Y[signal,:]),np.max(Y[signal,:])],'k--')\n",
        "plt.legend()\n",
        "\n",
        "print('Between t=0 and t=100, we feed the real values')\n",
        "print('From t=100, we feed the estimated values (forecasting)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCNJJ3nc6vwU"
      },
      "source": [
        "Observe that, during forecasting, sometimes the model quickly diverges from the target. That is **expected** we are sampling from the generative model and it is likely that do not get exactly the same sample! Particularly in RNN, since they have short memory.\n",
        "\n",
        "## LSTMs\n",
        "\n",
        "Lets study how an LSTM would perform in this context. You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n",
        "\n",
        "```python\n",
        "self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "```\n",
        "\n",
        "where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. If **stacked LSTMs (n_layers>1) are used** we can automatically add dropout between LSTM layers with te parameter `dropout` with a specified probability.\n",
        "\n",
        "> **Exercise:** Complete the code for the following two classes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVJMEtCr6vwU"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers,sigma,drop_prob):\n",
        "        \n",
        "        # input size -> Dimension of the input signal\n",
        "        # outpusize -> Dimension of the output signal\n",
        "        # hidden_dim -> Dimension of the rnn state\n",
        "        # n_layers -> If >1, we are using a stacked RNN\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        \n",
        "        self.sigma = sigma\n",
        "\n",
        "        # define an RNN with specified parameters\n",
        "        # batch_first=True means that the first dimension of the input will be the batch_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # last, fully-connected layer\n",
        "        self.fc1 = # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x, h0=None, valid=False):\n",
        "        \n",
        "        '''\n",
        "        About the shape of the different tensors ...:\n",
        "        \n",
        "        - Input signal x has shape (batch_size, seq_length, input_size)\n",
        "        - The initialization of the LSTM hidden state is a tuple, containing two tensors of dimensions\n",
        "          (n_layers, batch_size, hidden_dim) each. The first tensor represents the LSTM hidden state \n",
        "          cell states. We can use the None value so internally they are initialized with 0s.\n",
        "        - The LSTM output shape is (batch_size, seq_length, hidden_size) \n",
        "\n",
        "        '''\n",
        "        \n",
        "        # If we use stacked LSTMs, we have to control the evaluation mode due to the dropout between LSTMs\n",
        "        if(valid):\n",
        "            self.eval()\n",
        "        else:\n",
        "            self.train()\n",
        "        \n",
        "        batch_size = x.size(0) # Number of signals N\n",
        "        seq_length = x.size(1) # T\n",
        "        \n",
        "        # get RNN outputs\n",
        "        # r_out is the sequence of states\n",
        "        # hidden is just the last state (we will use it for forecasting)\n",
        "    \n",
        "        r_out, hidden = self.lstm(x, h0)\n",
        "        \n",
        "        # shape r_out to be (seq_length, hidden_dim)    \n",
        "        r_out = r_out.reshape(-1, self.hidden_dim) \n",
        "        \n",
        "        output = self.fc1(# YOUR CODE HERE)\n",
        "        \n",
        "        noise = torch.randn_like(output)*sigma\n",
        "        \n",
        "        output += noise\n",
        "        \n",
        "        # reshape back to temporal structure\n",
        "        output = output.reshape([-1,seq_length,1])\n",
        "        \n",
        "        return output, hidden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZWw1OhT6vwU"
      },
      "source": [
        "class LSTM_extended(LSTM):\n",
        "    \n",
        "    #Your code here\n",
        "    \n",
        "    def __init__(self, num_data_train, num_iter, sequence_length,\n",
        "                 input_size, output_size, hidden_dim, n_layers, sigma, drop_prob=0.3, lr=0.001):\n",
        "        \n",
        "        super().__init__(input_size, output_size, hidden_dim, n_layers,sigma,drop_prob) \n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "        self.num_layers = n_layers\n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.num_train = num_data_train #Number of training signals\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.num_iter = num_iter\n",
        "        \n",
        "        self.criterion = #YOUR CODE HERE     \n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "           \n",
        "    def trainloop(self,x,y):\n",
        "        \n",
        "        # SGD Loop\n",
        "        \n",
        "        for e in range(int(self.num_iter)):\n",
        "        \n",
        "            self.optim.zero_grad() \n",
        "                \n",
        "            x = #YOUR CODE HERE \n",
        "\n",
        "            y = #YOUR CODE HERE \n",
        "\n",
        "            out,hid = #YOUR CODE HERE \n",
        "                \n",
        "            loss = #YOUR CODE HERE \n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            # This code helps to avoid vanishing exploiting gradients in RNNs\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
        "                \n",
        "            self.optim.step()\n",
        "                \n",
        "            self.loss_during_training.append(loss.item()/self.num_iter)\n",
        "\n",
        "            if(e % 50 == 0): # Every 10 iterations\n",
        "\n",
        "                print(\"Iteration %d. Training loss: %f\" %(e,self.loss_during_training[-1]))                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pylvfJY6vwV"
      },
      "source": [
        "> **Exercise:** Train the LSTM model for 200 iterations using the first 100 values of each signal. Use `hidden_dim=32`, `n_layers=1`, `lr=0.005`, and `sigma=1`. Recall that the target signal is stored in the variable `Y`.\n",
        "Note that with only one layer, the dropout probability parameter does not play any role (you will get a warning actually).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3sxS4W6vwV"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itx-8NV7DLe9"
      },
      "source": [
        "> **Exercise:** Plot the loss function along training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyqKYPdLDKeo"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksKgXiyd6vwV"
      },
      "source": [
        "> **Exercise:** Complete the code to visualize the LSTM prediction of the next value of the signal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtvBT0IL6vwV"
      },
      "source": [
        "# We first evaluate the model for the N signals up to time T_train = 100\n",
        "X_in = # YOUR CODE HERE\n",
        "\n",
        "o,h = my_lstm.forward(# YOUR CODE HERE)\n",
        "\n",
        "output_lstm = o.detach().numpy().reshape([N,-1])\n",
        "\n",
        "offset = 50\n",
        "\n",
        "signal = 6 # From 1 to N (you can play with this)\n",
        "\n",
        "# Plot the first training signal and the target\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.arange(T_train-offset,T_train,1), X[signal,T_train-offset:T_train], 'r.--', label='input, x',ms=10) # x\n",
        "plt.plot(np.arange(T_train-offset,T_train,1), Y[signal,T_train-offset:T_train], 'b.-', label='target, y',ms=10) # x\n",
        "plt.plot(np.arange(T_train-offset,T_train,1), output_lstm[signal,T_train-offset:T_train], 'k.-', label='LSTM output',ms=10) # y\n",
        "\n",
        "\n",
        "plt.legend(loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr-tJUsL6vwV"
      },
      "source": [
        "> **Exercise:** Complete the code to visualize the LSTM forecasting. Plot the LSTM vs RNN vs target for a few signals and discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6oDZ7_z6vwV"
      },
      "source": [
        "# We take the last RNN output \n",
        "current_input = o[:,-1,:].view([N,1,1]) #Note that current input only contains one observation for each of the N signals\n",
        "# We take the last RNN state\n",
        "current_state = h\n",
        "\n",
        "forecast_lstm = np.zeros([N,T-T_train])\n",
        "\n",
        "for t in range(T-T_train):\n",
        "    \n",
        "    # ... and feed them as input and initial state\n",
        "    \n",
        "    current_input,current_state = # YOUR CODE HERE\n",
        "    \n",
        "    forecast_lstm[:,t] = current_input.detach().numpy().reshape([-1,])\n",
        "    \n",
        "final_lstm_reconstruct = np.hstack([output_lstm,forecast_lstm])\n",
        "\n",
        "# We plot the signal and the target before and after forecasting\n",
        "\n",
        "signal = 6\n",
        "\n",
        "plt.plot(np.arange(0,T-1,1), Y[signal,:-1].reshape([-1]), 'b.-', label='target, y',ms=10) \n",
        "plt.plot(np.arange(0,T-1,1), final_lstm_reconstruct[signal,:-1], 'g.-', label='LSTM output',ms=10) \n",
        "plt.plot(np.arange(0,T-1,1), final_rnn_reconstruct[signal,:-1], 'r-', label='RNN output',ms=10) \n",
        "plt.plot([T_train,T_train],[np.min(Y[signal,:]),np.max(Y[signal,:])],'k--')\n",
        "plt.legend()\n",
        "\n",
        "print('Between t=0 and t=100, we feed the real values')\n",
        "print('From t=100, we feed the estimated values (forecasting)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZF7__i66vwW"
      },
      "source": [
        "You should observe that the LSTM is able to keep track of the real signal during forecasting for a longer period of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rpsyWVL6vwW"
      },
      "source": [
        "> **Exercise (Optional):** Train an LSTM with 3 layers and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGTOjVdQ6vwW"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
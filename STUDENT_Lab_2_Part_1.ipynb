{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "STUDENT_Lab_2_Part_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpBFlcP7swRx"
      },
      "source": [
        "# Lab 2 (Part 1): Image classification with MLPs\n",
        "\n",
        "\n",
        "------------------------------------------------------\n",
        "*Neural Networks. Bachelor in Data Science and Engineering*\n",
        "\n",
        "*Pablo M. Olmos pamartin@ing.uc3m.es*\n",
        "\n",
        "*Aurora Cobo Aguilera acobo@tsc.uc3m.es*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "In this second lab of the course, you will implement an image classifier using MLPs. We will use the MNIST dataset, which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQNlldXHswR4"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "\n",
        "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\", width=400, height=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOmFf1dxswR7"
      },
      "source": [
        "Our goal is to build a neural network that can take one of these images and predict the digit in the image.\n",
        "\n",
        "Note: a big part of the following material is a personal wrap-up of [Facebook's Deep Learning Course in Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188). So all credit goes for them!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NotjAZRbswR8"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  #To get figures with high quality!\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbrnOI3iswR8"
      },
      "source": [
        "## Part I. Download MNIST with `torchvision`\n",
        "\n",
        "First up, we need to get our dataset. This is provided through the `torchvision` package. The [torchvision package](https://pytorch.org/docs/stable/torchvision/index.html) consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "\n",
        "\n",
        "The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZL9ht0yswR9"
      },
      "source": [
        "### Run this cell\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Download and load the training  data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKnKfmN8swSA"
      },
      "source": [
        "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. Later, we'll use this to loop through the dataset for training, like\n",
        "\n",
        "```python\n",
        "for image, label in trainloader:\n",
        "    ## do things with images and labels\n",
        "```\n",
        "\n",
        "You'll notice I created the `trainloader` with a batch size of 64, and `shuffle=True`. The batch size is the number of images we get in one iteration from the data loader and pass through our network, often called a *batch*. And `shuffle=True` tells it to **shuffle the dataset every time we start going through the data loader again**. But here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size `(64, 1, 28, 28)`. So, 64 images per batch, **1 color channel**, and 28x28 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTDbQscJswSF"
      },
      "source": [
        "dataiter = iter(trainloader)   #To iterate through the dataset\n",
        "\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPs3rnMtswSP"
      },
      "source": [
        "This is what one of the images looks like. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMEjDWhUswSP"
      },
      "source": [
        "plt.imshow(images[1].numpy().reshape([28,28]), cmap='Greys_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fCFp_tXswSQ"
      },
      "source": [
        "## Part II. Train a multi-class Logistic Regressor\n",
        "\n",
        "Our first goal is to train a multi-class logistic regressor to evaluate how good it can do in both the training and the test sets. \n",
        "\n",
        "The following code is **almost identical** to the one you used for Lab 1 except for two small details:\n",
        "\n",
        "- We are training a LR classifier with 10 different outputs that implements a **softmax** non-linear function (instead of a binary LR with a sigmoid). \n",
        "\n",
        "- We are using the MNIST database loaded above.\n",
        "\n",
        "We first define the Multi-class Logistic Regressor class\n",
        "\n",
        "> **Exercise**: Complete the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFRQTI1NswSQ"
      },
      "source": [
        "class Multi_LR(nn.Module):\n",
        "    def __init__(self,dimx,nlabels): #Nlabels will be 10 in our case\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output = nn.Linear(dimx,nlabels)\n",
        "    \n",
        "         \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)   # NEW w.r.t Lab 1. dim is the dimension along which \n",
        "                                                 #Softmax will be computed (so every slice along dim will sum to 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        #YOUR CODE HERE\n",
        "        #YOUR CODE HERE\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I037PvoDswSY"
      },
      "source": [
        "Note that we use `nn.LogSoftmax` instead of `nn.Softmax()`. In many cases, softmax gives you probabilities which will often be very close to zero or one but floating-point numbers can't accurately represent values near zero or one ([read more here](https://docs.python.org/3/tutorial/floatingpoint.html)). It's usually best to avoid doing calculations with probabilities, typically we use log-probabilities.  The cross entropy loss is obtained by combining `nn.LogSoftmax` with the negative loss likelihood loss `nn.NLLLoss()`.\n",
        "\n",
        "Alternatively, we can use [`nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss). **This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class.**\n",
        "\n",
        "This means we need to pass in the raw output of our network into the loss, not the output of the softmax function. This raw output is usually called the *logits* or *scores*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVQK2JB-swSq"
      },
      "source": [
        "Now we implement an extension to the class above (which inheritates from `Multi_LR`) that includes a training method.  One thing, note that MNIST images are of dimension $28\\times28=784$. To feed this image as the input to a `nn.Linear` layer, it has to be converted to a $784\\times 1$ input tensor. \n",
        "\n",
        "> **Exercise**: Complete the following code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqh_KVsrswWC"
      },
      "source": [
        "''' This class inherits from the `Multi_LR` class. So it has the same atributes\n",
        "and methods, and some others that we will add. \n",
        "'''\n",
        "class Multi_LR_extended(Multi_LR):\n",
        "    \n",
        "    def __init__(self,dimx,nlabels,epochs=100,lr=0.001):\n",
        "        \n",
        "        super().__init__(dimx,nlabels)  #To initialize `Multi_LR`!\n",
        "        \n",
        "        self.lr = lr #Learning Rate\n",
        "        \n",
        "        self.optim = optim.Adam(self.parameters(), self.lr)\n",
        "        \n",
        "        self.epochs = epochs\n",
        "        \n",
        "        self.criterion = nn.NLLLoss()               # NEW w.r.t Lab 1\n",
        "        \n",
        "        # A list to store the loss evolution along training\n",
        "        \n",
        "        self.loss_during_training = [] \n",
        "        \n",
        "    def train(self,trainloader):\n",
        "        \n",
        "        # Optimization Loop\n",
        "        \n",
        "        for e in range(int(self.epochs)):\n",
        "            \n",
        "            # Random data permutation at each epoch\n",
        "            \n",
        "            running_loss = 0.\n",
        "            \n",
        "            for images, labels in trainloader:              # NEW w.r.t Lab 1\n",
        "        \n",
        "                self.optim.zero_grad()  #TO RESET GRADIENTS!\n",
        "            \n",
        "                out = self.forward(images.view(images.shape[0], -1))\n",
        "\n",
        "                #Your code here (multiple lines)\n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "            self.loss_during_training.append(running_loss/len(trainloader))\n",
        "\n",
        "            if(e % 1 == 0): # Every 10 epochs\n",
        "\n",
        "                print(\"Training loss after %d epochs: %f\" \n",
        "                      %(e,self.loss_during_training[-1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktixYYWKswWE"
      },
      "source": [
        "Ok that was easy, wasn't it? Lets now train the multi-class LR and evaluate the performance in both the training and the test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BC1EPAWxswWR"
      },
      "source": [
        "my_LR = Multi_LR_extended(dimx=784,nlabels=10,epochs=5,lr=1e-3)\n",
        "\n",
        "my_LR.train(trainloader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqeATXGKswWc"
      },
      "source": [
        "plt.plot(my_LR.loss_during_training,'-b',label='Cross Entropy Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaJXOWVrswWd"
      },
      "source": [
        "To evaluate the performance across the entire test dataset, we will implement a for loop using `testloader` and compute errors per mini-batch. The following code will do the work:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "q3gZhHgEswWd"
      },
      "source": [
        "loss = 0\n",
        "accuracy = 0\n",
        "\n",
        "# Turn off gradients for validation, saves memory and computations\n",
        "with torch.no_grad():\n",
        "\n",
        "    for images,labels in testloader:\n",
        "        \n",
        "        logprobs = my_LR.forward(images.view(images.shape[0], -1)) # We use a log-softmax, so what we get are log-probabilities\n",
        "        \n",
        "        top_p, top_class = logprobs.topk(1, dim=1)\n",
        "        equals = (top_class == labels.view(images.shape[0], 1))\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "\n",
        "print(\"Test Accuracy %f\" %(accuracy/len(testloader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rqvX4nwswWd"
      },
      "source": [
        "With the probabilities, we can get the most likely class using the `probs.topk` method. This returns the $k$ highest values. Since we just want the most likely class, we can use `probs.topk(1)`. This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index.\n",
        "\n",
        "The line \n",
        "```python\n",
        "(top_class == labels.view(images.shape[0], 1))\n",
        "```\n",
        "returns a boolean vector of `True/False` values, indicanting whether `top_class` is equal to `labels` at every position. Finally, with the line\n",
        "\n",
        "```python\n",
        "equals.type(torch.FloatTensor)\n",
        "```\n",
        "we transform it to real a vector in which `True --> 1.0` and `False --> 0.0`, where we can compute the mean using `torch.mean()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnjduGyGswWd"
      },
      "source": [
        "> **Excercise** Modify the code of the `Multi_LR_extended` class so it incorporates a method to evaluate the performance in either the train set or the test set (Use a single method with the proper inputs!). Compute the train/test accuracy using such a method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecwH04ddswWe"
      },
      "source": [
        "#YOUR CODE HERE     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv5wgZmGswWs"
      },
      "source": [
        "#YOUR CODE HERE    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuIP00JjswWy"
      },
      "source": [
        "Observe that both values are indeed similar, indicating that the model is not overfitting.  \n",
        "\n",
        "Let's check the values for the weight matrix. For a simpler visualization, we will plot the histogram of all the values in the weight matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2EKx21o7swWy"
      },
      "source": [
        "plt.hist(my_LR.output.weight.detach().numpy().reshape([-1,]),50) # Modify my_LR by the name of the object you defined above\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPpJAheHswW1"
      },
      "source": [
        "> **Exercise**: Plot the histogram of the gradients of the loss function w.r.t. every parameter in the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGaIuM5-swW3"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLcDDrWaswW_"
      },
      "source": [
        "As we can see most of the gradients are almost zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5jNW3gpswXA"
      },
      "source": [
        "## Part III. Train a MLP to do the same job\n",
        "\n",
        "Modify the code you have just implemented for a MLP is straightforward. Assume we want to train a MLP with three layers, all using rectified linear units (RELU)s as non-linear activations (except the last layer, that uses a Softmax). The first layer has 128 hidden units and the second 64 of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9csiqEtRswXA"
      },
      "source": [
        "Image(url= \"https://pytorch.org/docs/stable/_images/ReLU.png\", width=300, height=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx5eXbgUswXC"
      },
      "source": [
        "As with the LR, we create a small class defining the model and then a larger class than inherites from it to incorporate methods to perform both training and model evaluation.\n",
        "\n",
        "> **Exercise**: Complete the code for the following class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6t31rHkswXD"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,dimx,hidden1,hidden2,nlabels): #Nlabels will be 10 in our case\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output1 = nn.Linear(dimx,hidden1)\n",
        "        \n",
        "        self.output2 = nn.Linear(hidden1,hidden2)\n",
        "        \n",
        "        self.output3 = nn.Linear(hidden2,nlabels)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)                                                             \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.output1(x)\n",
        "        x = self.relu(x)\n",
        "        x = #YOUR CODE HERE\n",
        "        x = #YOUR CODE HERE\n",
        "        x = #YOUR CODE HERE\n",
        "        x = #YOUR CODE HERE\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8gaTsyxswXD"
      },
      "source": [
        "> **Excercise** Create a class `MLP_extended` that incorporates two methods to the former class. One to perform training and one to perform model evaluation. It is just **one line of code** diferent from the code you have done above for the multi-class LR. This is why I like class and structure my code this way!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaWIKkm2swXH"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtZeWB7GswXH"
      },
      "source": [
        "Train the model for 10 epochs and compute the train/test performance. Then plot the loss during trianing. How does it compare with the Logistic Regressor?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvkWo2-4swXH"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNJDQb-XtUic"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpyiPy8YswXI"
      },
      "source": [
        "Wow! Performace is almost perfect with a naive Neural Network!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY8GwkppswXI"
      },
      "source": [
        "> **Exercise:** Lets visualize the activations at the ouput of the first layer for a minibatch of test images. This will help to identify possible unused hidden units (always activated/deactivated) and correlated hidden units, e.g. redundant units. Complete the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btogzl5WswXM"
      },
      "source": [
        "# First, we load a test minibatch\n",
        "\n",
        "x_test,y_test = next(iter(testloader)) \n",
        "\n",
        "# Then, we evaluate the output of the first layer of the network for that mini-batch\n",
        "\n",
        "activations = # YOUR CODE HERE\n",
        "\n",
        "# We plot the matrix using matplotlib\n",
        "\n",
        "plt.matshow(activations)\n",
        "\n",
        "plt.colorbar()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-PrNG81swXN"
      },
      "source": [
        "Based on the plot, do you think there are unsued hidden units in the hidden layer? They are characterized by units that are always active (very high values) or unactive (almost zero values). Plot the variance of the hidden units across the test mini-batch to better visualize these unactive hidden units. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdzv8DJeswXN"
      },
      "source": [
        "plt.plot(np.var(activations,0))\n",
        "plt.grid()\n",
        "plt.xlabel('Hidden Unit')\n",
        "plt.ylabel('Activation Variance')\n",
        "\n",
        "print(\"There are {0:d} hidden units that are unactive\".format(np.sum(np.var(activations,0)<=0.1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljUPCy3pswXO"
      },
      "source": [
        "> **Exercise**: Retrain the model reducing accordingly the dimension of the first hidden layer. For that model, repeat the analysis to the activations of both the first and the second layer. \n",
        ">\n",
        ">You will notice that in general, unsued activations are prominent in the first layer compared to the second one. This is in general the case for any NN, as the **loss function is more sensitive to parameter variations in the last layers**, and hence gradients are higher in magnitude. On the contrary, the **loss function is less senstive to parameter variations in the first layers** and hence only very relevant parameters are trained (they influence more in the loss function), while many others vary very little w.r.t. initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lru662ntswXP"
      },
      "source": [
        "#YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JEcRTXsswXR"
      },
      "source": [
        "activations_1 = # YOUR CODE HERE\n",
        "\n",
        "# Then, we evaluate the output of the first layer of the network for that mini-batch\n",
        "activations_2 = # YOUR CODE HERE\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2,figsize=(16, 8))\n",
        "im = ax[0,0].matshow(activations_1)\n",
        "ax[0,0].set_title('Activations in the first layer\\n')\n",
        "\n",
        "ax[0,1].matshow(activations_2)\n",
        "ax[0,1].set_title('Activations in the second layer\\n')\n",
        "\n",
        "ax[1,0].plot(np.var(activations_1,0))\n",
        "ax[1,0].set_title('Activation variance in the first layer\\n')\n",
        "ax[1,0].grid()\n",
        "\n",
        "ax[1,1].plot(np.var(activations_2,0))\n",
        "ax[1,1].set_title('Activation variance in the second layer\\n')\n",
        "ax[1,1].grid()\n",
        "\n",
        "print(\"In the first layer, there are {0:d} hidden units that are unactive\".format(np.sum(np.var(activations_1,0)<=0.1)))\n",
        "\n",
        "print(\"In the second layer, there are {0:d} hidden units that are unactive\".format(np.sum(np.var(activations_2,0)<=0.1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PItlRumqwCTs"
      },
      "source": [
        "> **Exercise**: Plot the histogram of the gradient of the loss function w.r.t. the parameters in the model for the first and the last layers and compare them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYPd644LwBgZ"
      },
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPGbWRCoswXS"
      },
      "source": [
        "### Part IV. Saving and restoring the model\n",
        "\n",
        "Finally, we will show you how to save and load models (i.e. values of the parameters) with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data.\n",
        "\n",
        "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
        "\n",
        "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yigg0Q8vswXV"
      },
      "source": [
        "print(\"Our model: \\n\\n\", my_MLP, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", my_MLP.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5F1Sv0vswXW"
      },
      "source": [
        "The simplest thing to do is saving the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeTsBI4lswXX"
      },
      "source": [
        "torch.save(my_MLP.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvKmC5KswXY"
      },
      "source": [
        "Then we can load the state dict with `torch.load`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuCEQZieswXb"
      },
      "source": [
        "state_dict = torch.load('checkpoint.pth')\n",
        "print(state_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P84hjtUbswXc"
      },
      "source": [
        "And to load the state dict in to the network, you do `my_MLP.load_state_dict(state_dict)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VthjcEyswXc"
      },
      "source": [
        "my_MLP.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEaKOSqUswXd"
      },
      "source": [
        "**Important:** `load_state_dict` will raise an error if the architecture of the network is different from the one saved in the pth file. For example, if we define the following model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3M5aVqIswXd"
      },
      "source": [
        "my_MLP2 = MLP_extended(dimx=784,hidden1=256,hidden2=128,nlabels=10,epochs=10,lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS6K_DLZswXg"
      },
      "source": [
        "which differs from `my_MLP` in the dimension of the hidden layers, we will get an error if we call the method  `load_state_dict(state_dict)`.\n",
        "\n",
        "> **Exercise:** Check that you get an error when trying to initialize my_MLP2 from `state_dict` using the method `load_state_dict`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogqN-yO5swXg"
      },
      "source": [
        "my_MLP2.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2tWyWfbswXg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PRePU2kswXh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}